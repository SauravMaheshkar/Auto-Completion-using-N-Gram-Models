{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N-gram Model",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psych0man/Auto-Completion-using-N-Gram-Models/blob/master/N_gram_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHfL7bUmhdn1",
        "colab_type": "text"
      },
      "source": [
        "# Importing Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ferVSHKAgsN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEVGficVhzNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.data.path.append('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnBR3vgHkLow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9b8aac26-71b3-45a8-8437-94916fc13b4e"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBqL7EdgiCb-",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxfOCDGDkXGV",
        "colab_type": "text"
      },
      "source": [
        "**Reading the Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcztRHMVh4kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"text.txt\", \"r\") as f:\n",
        "  data = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rDjbwOVkZ5D",
        "colab_type": "text"
      },
      "source": [
        "**Converting corpus into sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnCmoxYliY4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def obtain_sentences(data):\n",
        "  sentences = data.split(\"\\n\")\n",
        "  sentences = [s.strip() for s in sentences]\n",
        "  sentences = [s for s in sentences if len(s) > 0]\n",
        "  return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFR4qCdqkeoo",
        "colab_type": "text"
      },
      "source": [
        "**Tokenizing the Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6STE_sztjUAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_the_corpus(sentences):\n",
        "  tokenized_sentences = []\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "    tokenized_sentences.append(tokenized_sentence)\n",
        "  return tokenized_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yreEgmlHjsXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = obtain_sentences(data)\n",
        "tokenized_sentences = tokenize_the_corpus(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ODLNwQ1kkJ6",
        "colab_type": "text"
      },
      "source": [
        "**Splitting the Corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysFByyWWkHXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = int(len(tokenized_sentences) * 0.8)\n",
        "train_data = tokenized_sentences[0:train_size]\n",
        "test_data = tokenized_sentences[train_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG0n7iNvlCXP",
        "colab_type": "text"
      },
      "source": [
        "Instead of using every word of the corpus we generally use the words which occur most frequently "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ipu8BZLk3Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_the_words(sentences):\n",
        "\n",
        "  word_counts = {}\n",
        "  for sentence in sentences:\n",
        "    for token in sentence:\n",
        "      if token not in word_counts.keys():\n",
        "        word_counts[token] = 1\n",
        "      else:\n",
        "        word_counts[token] += 1\n",
        "  return word_counts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHrvPbadloOT",
        "colab_type": "text"
      },
      "source": [
        "**Handling OOV Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnRIaZe5ll4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def handling_oov(tokenized_sentences, count_threshold):\n",
        "  closed_vocabulary = []\n",
        "  words_count = count_the_words(tokenized_sentences)\n",
        "  for word, count in words_count.items():\n",
        "    if count >= count_threshold :\n",
        "      closed_vocabulary.append(word)\n",
        "\n",
        "  return closed_vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAKDAGQ-mWqi",
        "colab_type": "text"
      },
      "source": [
        "**Converting OOV to UNK Tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g40WS3IkmPSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\"):\n",
        "  vocabulary = set(vocabulary)\n",
        "  new_tokenized_sentences = []\n",
        "  for sentence in tokenized_sentences:\n",
        "    new_sentence = []\n",
        "    for token in sentence:\n",
        "      if token in vocabulary:\n",
        "        new_sentence.append(token)\n",
        "      else:\n",
        "        new_sentence.append(unknown_token)\n",
        "    new_tokenized_sentences.append(new_sentence)\n",
        "  return new_tokenized_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWMjFcovoc4J",
        "colab_type": "text"
      },
      "source": [
        "**Final Steps**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEhjln_dms5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(train_data, test_data, count_threshold):\n",
        "  vocabulary = handling_oov(train_data, count_threshold)\n",
        "  new_train_data = unk_tokenize(train_data, vocabulary)\n",
        "  new_test_data = unk_tokenize(test_data, vocabulary)\n",
        "\n",
        "  return new_train_data, new_test_data, vocabulary "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtL6gVpVoCEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_freq = 2\n",
        "final_train, final_test, vocabulary = preprocess(train_data, test_data, min_freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d06nKPmCofTU",
        "colab_type": "text"
      },
      "source": [
        "# N-Gram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjd5IGsLoiE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\"):\n",
        "  n_grams = {}\n",
        "  for sentence in data:\n",
        "    sentence = [start_token]*n + sentence + [end_token]\n",
        "    sentence = tuple(sentence)\n",
        "\n",
        "    m = len(sentence) if n==1 else len(sentence)-1\n",
        "    for i in range(m):\n",
        "      n_gram = sentence[i:i+n]\n",
        "      if n_gram in n_grams.keys():\n",
        "        n_grams[n_gram] += 1\n",
        "      else:\n",
        "        n_grams[n_gram] = 1\n",
        "  return n_grams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY_ET3VUqB96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0):\n",
        "  previous_n_gram = tuple(previous_n_gram)\n",
        "  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
        "  denom = previous_n_gram_count + k * vocabulary_size\n",
        "  nplus1_gram = previous_n_gram + (word,)\n",
        "  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
        "  num = nplus1_gram_count + k\n",
        "  prob = num / denom\n",
        "  return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlGHJJvtrtbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0):\n",
        "  previous_n_gram = tuple(previous_n_gram)\n",
        "  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "  vocabulary_size = len(vocabulary)\n",
        "  probabilities = {}\n",
        "  for word in vocabulary:\n",
        "    probability = prob_for_single_word(word, previous_n_gram, \n",
        "                                           n_gram_counts, nplus1_gram_counts, \n",
        "                                           vocabulary_size, k=k)\n",
        "    probabilities[word] = probability\n",
        "\n",
        "  return probabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB5q8BkrtG9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_matrix(nplus1_gram_counts, vocabulary):\n",
        "  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "  n_grams = []\n",
        "  for n_plus1_gram in nplus1_gram_counts.keys():\n",
        "    n_gram = n_plus1_gram[0:-1]\n",
        "    n_grams.append(n_gram)\n",
        "  n_grams = list(set(n_grams))\n",
        "\n",
        "  row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
        "  col_index = {word:j for j, word in enumerate(vocabulary)}\n",
        "    \n",
        "  nrow = len(n_grams)\n",
        "  ncol = len(vocabulary)\n",
        "  count_matrix = np.zeros((nrow, ncol))\n",
        "  for n_plus1_gram, count in nplus1_gram_counts.items():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        word = n_plus1_gram[-1]\n",
        "        if word not in vocabulary:\n",
        "            continue\n",
        "        i = row_index[n_gram]\n",
        "        j = col_index[word]\n",
        "        count_matrix[i, j] = count\n",
        "    \n",
        "  count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
        "  return count_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXU7cekVtgK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prob_matrix(nplus1_gram_counts, vocabulary, k):\n",
        "  countmatrix = count_matrix(nplus1_gram_counts, unique_words)\n",
        "  countmatrix += k\n",
        "  prob_matrix = countmatrix.div(countmatrix.sum(axis=1), axis=0)\n",
        "  return prob_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ri_qGQlt075",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation using Perplexity "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTawCCQAtwpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity(sentence, n_gram_counts, nplus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    n = len(list(n_gram_counts.keys())[0]) \n",
        "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
        "    sentence = tuple(sentence)\n",
        "    \n",
        "    N = len(sentence)\n",
        "\n",
        "    product_pi = 1.0\n",
        "\n",
        "    for t in range(n, N): \n",
        "\n",
        "        n_gram = sentence[t-n:t]\n",
        "\n",
        "        word = sentence[t]\n",
        "\n",
        "        probability = prob_for_single_word(word,n_gram, n_gram_counts, nplus1_gram_counts, len(unique_words), k=1)\n",
        "\n",
        "        product_pi *= 1 / probability\n",
        "\n",
        "    perplexity = product_pi**(1/float(N))\n",
        " \n",
        "    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovXG-N7Gv5Xl",
        "colab_type": "text"
      },
      "source": [
        "# Putting Everything Together: Auto-Complete System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkZsxv2qv2nG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
        "    n = len(list(n_gram_counts.keys())[0]) \n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
        "\n",
        "    suggestion = None\n",
        "    max_prob = 0\n",
        "\n",
        "    for word, prob in probabilities.items():\n",
        "        if start_with != None: \n",
        "            if not word.startswith(start_with):\n",
        "                continue \n",
        "\n",
        "        if prob > max_prob: \n",
        "\n",
        "            suggestion = word\n",
        "            max_prob = prob\n",
        "\n",
        "    return suggestion, max_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRtQhwbbwsKR",
        "colab_type": "text"
      },
      "source": [
        "**Extending to Multiple Suggestions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOXp3dIZwph2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "    model_counts = len(n_gram_counts_list)\n",
        "    suggestions = []\n",
        "    for i in range(model_counts-1):\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
        "        \n",
        "        suggestion = auto_complete(previous_tokens, n_gram_counts,\n",
        "                                    nplus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "        suggestions.append(suggestion)\n",
        "    return suggestions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAkjLJ9QxJMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1, 6):\n",
        "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
        "    n_model_counts = count_n_grams(final_train, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI0xNNqzw1FX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fe73ab89-5d31-46f7-db75-667ab8c1bbdb"
      },
      "source": [
        "previous_tokens = [\"i\", \"am\", \"to\"]\n",
        "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The previous words are ['i', 'am', 'to'], the suggestions are:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('be', 0.02698476197403353),\n",
              " ('go', 0.0001342822613132805),\n",
              " ('have', 0.00013432735576600176),\n",
              " ('how', 6.717270101430779e-05)]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}